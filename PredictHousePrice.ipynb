{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/openai/glide-text2im\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:41:28.385284Z","iopub.execute_input":"2022-01-13T21:41:28.385633Z","iopub.status.idle":"2022-01-13T21:41:44.485778Z","shell.execute_reply.started":"2022-01-13T21:41:28.385533Z","shell.execute_reply":"2022-01-13T21:41:44.484877Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom IPython.display import display\nimport torch as th\n\nfrom glide_text2im.download import load_checkpoint\nfrom glide_text2im.model_creation import (\n    create_model_and_diffusion,\n    model_and_diffusion_defaults,\n    model_and_diffusion_defaults_upsampler\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:46:19.465554Z","iopub.execute_input":"2022-01-13T21:46:19.465908Z","iopub.status.idle":"2022-01-13T21:46:20.918210Z","shell.execute_reply.started":"2022-01-13T21:46:19.465877Z","shell.execute_reply":"2022-01-13T21:46:20.917479Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Generate images from text using OpenAI GLIDE\n#GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models\n\n#https://github.com/openai/glide-text2im\n#https://arxiv.org/abs/2112.10741\n#!pip install git+https://github.com/openai/glide-text2im\n# Code adapted from: \n# https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb\n\n# Sampling parameters\n# Tune upsample_temp to control the sharpness of 256x256 images.\n# A value of 1.0 is sharper, but sometimes results in grainy artifacts.\nupsample_temp = 0.997\nbatch_size = 1\nguidance_scale = 3.0\n\ndef open_ai_glide(text_string_to_generate_image_from):\n    # This notebook supports both CPU and GPU.\n    # On CPU, generating one sample may take on the order of 20 minutes.\n    # On a GPU, it should be under a minute.\n    has_cuda = th.cuda.is_available()\n    device = th.device('cpu' if not has_cuda else 'cuda')\n    # Create base model.\n    options = model_and_diffusion_defaults()\n    options['use_fp16'] = has_cuda\n    options['timestep_respacing'] = '100' # use 100 diffusion steps for fast sampling\n    model, diffusion = create_model_and_diffusion(**options)\n    model.eval()\n    if has_cuda:\n        model.convert_to_fp16()\n    model.to(device)\n    model.load_state_dict(load_checkpoint('base', device))\n    print('total base parameters', sum(x.numel() for x in model.parameters()))\n    # Create upsampler model.\n    options_up = model_and_diffusion_defaults_upsampler()\n    options_up['use_fp16'] = has_cuda\n    options_up['timestep_respacing'] = 'fast27' # use 27 diffusion steps for very fast sampling\n    model_up, diffusion_up = create_model_and_diffusion(**options_up)\n    model_up.eval()\n    if has_cuda:\n        model_up.convert_to_fp16()\n    model_up.to(device)\n    model_up.load_state_dict(load_checkpoint('upsample', device))\n    print('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))\n    def show_images(batch: th.Tensor):\n        \"\"\" Display a batch of images inline. \"\"\"\n        scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n        reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n        display(Image.fromarray(reshaped.numpy()))\n        \n    ##############################\n    # Sample from the base model #\n    ##############################\n\n    # Create the text tokens to feed to the model.\n    tokens = model.tokenizer.encode(prompt)\n    tokens, mask = model.tokenizer.padded_tokens_and_mask(\n        tokens, options['text_ctx']\n    )\n\n    # Create the classifier-free guidance tokens (empty)\n    full_batch_size = batch_size * 2\n    uncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask(\n        [], options['text_ctx']\n    )\n\n    # Pack the tokens together into model kwargs.\n    model_kwargs = dict(\n        tokens=th.tensor(\n            [tokens] * batch_size + [uncond_tokens] * batch_size, device=device\n        ),\n        mask=th.tensor(\n            [mask] * batch_size + [uncond_mask] * batch_size,\n            dtype=th.bool,\n            device=device,\n        ),\n    )\n\n    # Create a classifier-free guidance sampling function\n    def model_fn(x_t, ts, **kwargs):\n        half = x_t[: len(x_t) // 2]\n        combined = th.cat([half, half], dim=0)\n        model_out = model(combined, ts, **kwargs)\n        eps, rest = model_out[:, :3], model_out[:, 3:]\n        cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)\n        half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n        eps = th.cat([half_eps, half_eps], dim=0)\n        return th.cat([eps, rest], dim=1)\n\n    # Sample from the base model.\n    model.del_cache()\n    samples = diffusion.p_sample_loop(\n        model_fn,\n        (full_batch_size, 3, options[\"image_size\"], options[\"image_size\"]),\n        device=device,\n        clip_denoised=True,\n        progress=True,\n        model_kwargs=model_kwargs,\n        cond_fn=None,\n    )[:batch_size]\n    model.del_cache()\n    \n    ##############################\n    # Upsample the 64x64 samples #\n    ##############################\n\n    tokens = model_up.tokenizer.encode(prompt)\n    tokens, mask = model_up.tokenizer.padded_tokens_and_mask(\n        tokens, options_up['text_ctx']\n    )\n\n    # Create the model conditioning dict.\n    model_kwargs = dict(\n        # Low-res image to upsample.\n        low_res=((samples+1)*127.5).round()/127.5 - 1,\n\n        # Text tokens\n        tokens=th.tensor(\n            [tokens] * batch_size, device=device\n        ),\n        mask=th.tensor(\n            [mask] * batch_size,\n            dtype=th.bool,\n            device=device,\n        ),\n    )\n\n    # Sample from the base model.\n    model_up.del_cache()\n    up_shape = (batch_size, 3, options_up[\"image_size\"], options_up[\"image_size\"])\n    up_samples = diffusion_up.ddim_sample_loop(\n        model_up,\n        up_shape,\n        noise=th.randn(up_shape, device=device) * upsample_temp,\n        device=device,\n        clip_denoised=True,\n        progress=True,\n        model_kwargs=model_kwargs,\n        cond_fn=None,\n    )[:batch_size]\n    model_up.del_cache()\n    \n    # Show the output\n    show_images(up_samples)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:46:25.962906Z","iopub.execute_input":"2022-01-13T21:46:25.963380Z","iopub.status.idle":"2022-01-13T21:46:25.992245Z","shell.execute_reply.started":"2022-01-13T21:46:25.963341Z","shell.execute_reply":"2022-01-13T21:46:25.991370Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"prompt = \"a hedgehog\"\nopen_ai_glide(prompt)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:46:30.855541Z","iopub.execute_input":"2022-01-13T21:46:30.855865Z","iopub.status.idle":"2022-01-13T22:02:32.592520Z","shell.execute_reply.started":"2022-01-13T21:46:30.855828Z","shell.execute_reply":"2022-01-13T22:02:32.591237Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"prompt = \"indian girl\"\nopen_ai_glide(prompt)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:04:31.638985Z","iopub.execute_input":"2022-01-13T22:04:31.639343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"prompt = \"little girl enjoying OpenAI images\"\nopen_ai_glides(prompt)","metadata":{}}]}